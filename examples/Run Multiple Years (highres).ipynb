{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cd7060d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Running FEISTY with time-series forcing from CESM\n",
    "\n",
    "This notebook doesn't really need to be a notebook, I'm hoping it can be converted to a script instead.\n",
    "All the parameter settings are handled in the first (non-`import`) cell, where we read in `feisty-config.TL319_t13.4p2z.001.yml`,\n",
    "though in the loop over years the start / end dates and file I/O parameters are changed.\n",
    "This was run on a casper login node with 4 GB of memory, but all data is kept distributed on the dask cluster.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b47d6f-2fee-4d26-9953-24ef92116cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b44b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dask\n",
    "import matplotlib.pyplot as plt\n",
    "import nc_time_axis  # needed for time series plot for some reason\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import yaml\n",
    "from dask.distributed import Client, wait\n",
    "from dask_jobqueue import PBSCluster\n",
    "\n",
    "import feisty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0bb673",
   "metadata": {},
   "source": [
    "## Configure run\n",
    "\n",
    "The FEISTY command `config_and_run_from_yaml()` needs a dictionary pointing to forcing streams and initial conditions.\n",
    "We provide a few `YAML` files containing acceptable configurations.\n",
    "There are also several parameters controlling how the run is set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28331de7-b41c-4b38-859e-a23166581b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read settings from YAML\n",
    "feisty_config_file = 'feisty-config.TL319_t13.4p2z.001.yml'\n",
    "with open(feisty_config_file) as f:\n",
    "    feisty_config_in = yaml.safe_load(f)\n",
    "\n",
    "outdir = os.path.join(os.path.sep, 'glade', 'scratch', os.environ['USER'], 'feisty_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0a8c81-4897-436a-a50b-c0e6267ecd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_forcing_dict(feisty_config, start_year, nyears, outdir='temp_forcing_streams'):\n",
    "    \"\"\"\n",
    "    The highres run has a large list of files to read for the forcing dataset,\n",
    "    and we want to create\n",
    "    \"\"\"\n",
    "\n",
    "    import copy\n",
    "\n",
    "    newfile_list = []\n",
    "    years = np.arange(np.maximum(1980, start_year - 1), np.minimum(2021, start_year + nyears + 1))\n",
    "    if not os.path.isdir(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    for file in feisty_config['forcing']['streams']:\n",
    "        newfile = f'{outdir}/{file.split(\"/\")[-1]}'\n",
    "        with open(file) as f:\n",
    "            forcing_dict = yaml.safe_load(f)\n",
    "        forcing_file_dict = {}\n",
    "        for n, forcing_file in enumerate(forcing_dict['files']):\n",
    "            forcing_file_dict[1980 + n] = forcing_file\n",
    "        forcing_dict['files'] = [forcing_file_dict[year] for year in years]\n",
    "        with open(newfile, 'w') as outfile:\n",
    "            yaml.dump(forcing_dict, outfile)\n",
    "        newfile_list.append(newfile)\n",
    "\n",
    "    feisty_config_out = copy.deepcopy(feisty_config)\n",
    "    feisty_config_out['forcing']['streams'] = newfile_list\n",
    "    return feisty_config_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fd19e8-afd1-4773-b41a-74a283a42633",
   "metadata": {},
   "source": [
    "## Set up Dask cluster\n",
    "\n",
    "Since the data in `ds` is chunked in (`nlat`, `nlon`), we use a `dask` cluster to configure the parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c476b9d-5430-4080-ab8c-74ea67cdc53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = 40  # GB\n",
    "dask.config.set({'distributed.dashboard.link': 'proxy/{port}/status'})\n",
    "cluster = PBSCluster(\n",
    "    memory=f'{mem} GB',\n",
    "    processes=1,\n",
    "    cores=1,\n",
    "    queue='casper',\n",
    "    walltime='1:30:00',\n",
    "    resource_spec=f'select=1:ncpus=1:mem={mem}GB',\n",
    "    log_directory='./dask-logs',\n",
    ")\n",
    "\n",
    "cluster.scale(feisty_config_in['num_workers'])\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fb5dba-96e0-40fb-b638-b856c06c5a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "min_workers = feisty_config_in.get('min_workers', feisty_config_in['num_workers'])\n",
    "worker_cnt = int(np.minimum(min_workers, feisty_config_in['num_workers']))\n",
    "print(f\"Waiting for {worker_cnt} workers (requested {feisty_config_in['num_workers']} total)\")\n",
    "client.wait_for_workers(worker_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3538b22f",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5754b-89a9-48fb-8e9c-ea53551d2173",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "start_year = 1981\n",
    "nyears = 1\n",
    "ds_list = list()\n",
    "feisty_config = modify_forcing_dict(feisty_config_in, start_year, nyears)\n",
    "for year in range(start_year, start_year + nyears):\n",
    "    print(f'Configuring FEISTY for {year}...')\n",
    "\n",
    "    # Start and End dates\n",
    "    feisty_config['start_date'] = f'{year}-01-01'\n",
    "    # feisty_config['end_date'] = f'{year}-12-31'\n",
    "    feisty_config['end_date'] = f'{year}-01-05'\n",
    "\n",
    "    # Initialize from restart (unless this is first year)\n",
    "    previous_restart = f'highres.{year-1}-12-31.zarr'\n",
    "    if os.path.exists(os.path.join(outdir, 'rest', previous_restart)):\n",
    "        feisty_config['initial_conditions'] = {\n",
    "            'root_dir': os.path.join(outdir, 'rest'),\n",
    "            'ic_file': previous_restart,\n",
    "        }\n",
    "\n",
    "    # Set up history and restart files for output\n",
    "    feisty_config['output']['hist_dir'] = os.path.join(outdir, 'hist')\n",
    "    feisty_config['output']['hist_file'] = f'highres.{year}.zarr'\n",
    "    feisty_config['output']['rest_dir'] = os.path.join(outdir, 'rest')\n",
    "    feisty_config['output']['rest_file'] = f'highres.{year}-12-31.zarr'\n",
    "\n",
    "    # map_blocks lets us run in parallel over our dask cluster\n",
    "    print(f'Running FEISTY for year {year}...')\n",
    "    ds_list.append(feisty.config_and_run_from_yaml(feisty_config))\n",
    "\n",
    "ds_out = xr.concat(ds_list, dim='time')\n",
    "ds_out[\"biomass\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8938bdd5-2f5d-4820-b859-94a737c40fec",
   "metadata": {},
   "source": [
    "<!-- ### Plotting -->\n",
    "\n",
    "Make a plot of `biomass` over time at a specified column,\n",
    "then do the same for `fish_yield`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14d8e3b-e4c2-4882-8154-61235b6298dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Select column for time series plot\n",
    "nlat = 91\n",
    "nlon = 3175\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "for group in ds_out.group.data:\n",
    "    ds_out['biomass'].isel(nlat=nlat, nlon=nlon).sel(group=group).plot()\n",
    "ax.set_ylim([5e-7, 50])\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_title(\"python\")\n",
    "plt.legend(ds_out.group.data, bbox_to_anchor=(1.025, 0.5), loc=6)\n",
    "fig.suptitle(f\"biomass at ({nlat}, {nlon})\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503bbf1a-1a26-4e54-85c0-69093ec1f6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Select column for time series plot\n",
    "nlat = 91\n",
    "nlon = 3175\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "plotted_fish = []\n",
    "for fish in ds_out.fish.data:\n",
    "    if np.nanmax(ds_out['fish_yield'].isel(nlat=nlat, nlon=nlon).sel(fish=fish).values) <= 0:\n",
    "        continue\n",
    "    plotted_fish.append(fish)\n",
    "    ds_out['fish_yield'].isel(nlat=nlat, nlon=nlon).sel(fish=fish).plot()\n",
    "ax.set_ylim([1e-6, 1e-2])\n",
    "ax.set_yscale(\"log\")\n",
    "plt.legend(plotted_fish, bbox_to_anchor=(1.025, 0.5), loc=6)\n",
    "fig.suptitle(f\"Fish yield at ({nlat}, {nlon})\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25f8b7d-aec4-47eb-b59b-739876acd328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "ds_out[\"biomass\"].isel(time=0, group=0).max().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34223f18-0fdf-412c-a05d-99016e372d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "ds_out[\"biomass\"].isel(time=180, group=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28c98dc-f7dd-407e-96e6-f38345e32896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "ds_out[\"biomass\"].isel(time=(nyears - 1) * 365 + 180, group=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c0d01b-041d-48ea-9d8c-742e520c4142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "ds_out[\"biomass\"].isel(time=-1, group=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4c3d65-7102-463c-b05f-b78b2c77f9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = xr.open_dataset(f'{outdir}/rest/highres.1980-12-31.zarr')\n",
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6c5673-b29d-4d21-ac69-22d2e5003a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.isnan(ds_test['bent_ic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2713ff1-e0ee-49bb-99cf-2b887709ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.isnan(ds_test['fish_ic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec63dc9-5548-469e-8e62-2c81867c3949",
   "metadata": {},
   "outputs": [],
   "source": [
    "25899528 / 3237441"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f067f5f3-eafc-4fbd-94f1-25736b4c9ada",
   "metadata": {},
   "source": [
    "ds_test['fish_ic'].isel(nfish=0).plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-dev-feisty]",
   "language": "python",
   "name": "conda-env-miniconda3-dev-feisty-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
